{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GITHUB final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanC777/AMSI-VRS-Project/blob/master/GITHUB_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "bgbo83VmERNv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "9ed8b0f0-9aec-4ddd-c955-fc45fa905351"
      },
      "cell_type": "code",
      "source": [
        "## Use older version of Keras so that embedding layer works\n",
        "\n",
        "!pip install keras==2.1.1"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.1.1 in /usr/local/lib/python3.6/dist-packages (2.1.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.1) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.1) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.1) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.1) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0ax-cyNxEVAb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "28b464e2-dade-48e8-ce80-552224bdf77c"
      },
      "cell_type": "code",
      "source": [
        "# Import all relevent packages and methods\n",
        "\n",
        "\n",
        "import math\n",
        "from math import sqrt\n",
        "import random\n",
        "from numpy.random import seed\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import special\n",
        "from sklearn import metrics, preprocessing\n",
        "from sklearn.preprocessing import  MinMaxScaler, StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import statsmodels.api as sm\n",
        "from keras.layers import Dense, Activation, Flatten, Embedding, Concatenate, Input, Dropout\n",
        "from keras.models import Sequential, Model\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers.core import Dropout\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from sklearn.datasets import make_regression, make_classification\n",
        "import statsmodels.formula.api as smf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
            "  from pandas.core import datetools\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "oXSjiqpCEubx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Regression\n"
      ]
    },
    {
      "metadata": {
        "id": "_T5CU7lLEyFE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Regression - 2 continuous, no cats**"
      ]
    },
    {
      "metadata": {
        "id": "4euoSUD1EY4T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Function to generate synthetic dataset\n",
        "def get_data1(observations, sigma):\n",
        "\n",
        "  x1 = np.random.randint(0,100, size = [observations,1]).astype(float)/10\n",
        "  x2 = np.random.randint(0,50, size = [observations,1]).astype(float)/10\n",
        "  \n",
        "  ## Manually set weights\n",
        "  w1 = 5\n",
        "  w2 = 2\n",
        "  \n",
        "  # Create error term which adds noise to data\n",
        "  e = np.random.normal(0, sigma, size = [observations,1]).astype(float) \n",
        "  \n",
        "  \n",
        "  x = np.hstack((x1,x2))\n",
        "  y = np.dot(x1**2,w1) + np.dot(x2,w2)  + e\n",
        "  \n",
        "  df1 = pd.DataFrame(x)\n",
        "  df2 = pd.DataFrame(y)\n",
        "  df = pd.concat([df1, df2], axis = 1)\n",
        "  \n",
        "  df.columns = [\"x1\", \"x2\",  \"y\"]\n",
        "\n",
        "  return(df)\n",
        "\n",
        "## Function to split dataframe into testing and training sets\n",
        "def split(df, test_size):\n",
        "  \n",
        "  test_size = int(np.round(test_size*df[\"y\"].size))\n",
        "  n = 1 - test_size\n",
        "  train = df.iloc[:n-1,:]\n",
        "  test = df.iloc[n-1:,:]\n",
        "  return(train, test)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ajy2Y9mrE9o3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Function to compare normal regression and neural network. \n",
        "## Train NN once and run model on random test set 30 times to get average test accuracy\n",
        "\n",
        "def compare_regression_data1(observations, sigma, test_size, vector, learn, epochs, rounds):\n",
        "  \n",
        "  train_ols_list = []\n",
        "  test_ols_list = []\n",
        "  \n",
        "  \n",
        "  train_nn_list = []\n",
        "  test_nn_list = []\n",
        "  \n",
        "  df = get_data1(observations = observations, sigma = sigma)\n",
        "  x = np.array(df.iloc[:, :-1]) \n",
        "  y = np.array(df.iloc[:,-1])\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(x,y, test_size= test_size) ## Split data into training and testing set\n",
        "  \n",
        "  ## Normalise data\n",
        "  scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "  scalarX.fit(X_train)\n",
        "  \n",
        "  X_train = scalarX.transform(X_train)\n",
        "  \n",
        "  X_test = scalarX.transform(X_test)\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  def build__regression_model(vector, learn):  ## Allow us to specify the size and number of neurons in the network with 'vector'\n",
        "      \n",
        "      model = Sequential()\n",
        "      \n",
        "      model.add(Dense(vector[0],  activation = 'relu', ## Fix input layer with relu activation function. Number of input neurons = number of features in the data\n",
        "                      input_dim=X_train.shape[1]) )\n",
        "      \n",
        "      for i in range(1,len(vector)-1):  ## Create loop which adds new layers of the sizes specified in 'vector'\n",
        "        \n",
        "         model.add(Dense(vector[i], activation= 'relu'))\n",
        "         \n",
        "        \n",
        "      model.add(Dense(1, activation = \"linear\")) ## Set final layer with output dim = 1, and linear activation function\n",
        "      \n",
        "    \n",
        "      optimizer = tf.train.AdamOptimizer(learn) \n",
        "\n",
        "      model.compile(loss='mse',             #Set loss as MSE, and use adam optimizer\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mse'])\n",
        "      return model\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  model = build__regression_model(vector = vector, learn = learn)\n",
        "  \n",
        "  # Store training stats\n",
        "  model.fit(X_train,y_train, epochs=epochs,\n",
        "                    validation_split=0.2, verbose=0)\n",
        "  \n",
        "  \n",
        "  [mse, acc] = model.evaluate(X_train,y_train, verbose=0)\n",
        "  y_pred_nn = model.predict(X_test).astype(np.float64)\n",
        "  \n",
        "  ## Take RMSE as final metric\n",
        "  train_nn = sqrt(mse)\n",
        "  \n",
        "  test_nn = sqrt(metrics.mean_squared_error(y_test, y_pred_nn))\n",
        "  \n",
        "  train_nn_list.append(train_nn)\n",
        "  test_nn_list.append(test_nn)\n",
        "  \n",
        "  \n",
        "  \n",
        "  ## Test on multiple test sets\n",
        "  for i in range(rounds):\n",
        "    ## OLS\n",
        "    df = get_data1(observations = observations, sigma = sigma)\n",
        "    train, test = split(df, test_size = test_size)\n",
        "    \n",
        "    results = smf.ols(formula = 'y ~ np.power(x1,2) + x2 ', data = sm.add_constant(train)).fit()\n",
        "    \n",
        "    train_pred_ols = results.fittedvalues\n",
        "    train_ols = sqrt(metrics.mean_squared_error(train_pred_ols, train[\"y\"]))\n",
        "    \n",
        "    test_pred_ols = results.predict(test)\n",
        "    test_ols = sqrt(metrics.mean_squared_error(test_pred_ols, test[\"y\"]))\n",
        "    \n",
        "    train_ols_list.append(train_ols)\n",
        "    test_ols_list.append(test_ols)\n",
        "  \n",
        "    ## NN\n",
        "\n",
        "    df = get_data1(observations = observations, sigma = sigma)\n",
        "    x = np.array(df.iloc[:, :-1])\n",
        "    y = np.array(df.iloc[:,-1])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size= test_size) ## Split data into training and testing set\n",
        "    \n",
        "    ## Normalise data\n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "    scalarX.fit(X_train)\n",
        "    \n",
        "    X_train = scalarX.transform(X_train)\n",
        "    \n",
        "    X_test = scalarX.transform(X_test)\n",
        "    \n",
        "    [mse, acc] = model.evaluate(X_train,y_train, verbose=0)\n",
        "    y_pred_nn = model.predict(X_test).astype(np.float64)\n",
        "    \n",
        "    ## Take RMSE as final metric\n",
        "    train_nn = sqrt(mse)\n",
        "    \n",
        "    test_nn = sqrt(metrics.mean_squared_error(y_test, y_pred_nn))\n",
        "    \n",
        "    train_nn_list.append(train_nn)\n",
        "    test_nn_list.append(test_nn)\n",
        "  \n",
        "  ## Find average MSE and standard deviation of error\n",
        "  \n",
        "  average_train_ols = np.mean(train_ols_list)\n",
        "  average_test_ols = np.mean(test_ols_list)\n",
        "  test_error_ols = np.std(test_ols_list)\n",
        "    \n",
        "  average_train_nn = np.mean(train_nn_list)\n",
        "  average_test_nn = np.mean(test_nn_list)  \n",
        "  test_error_nn = np.std(test_nn_list)\n",
        "\n",
        "  return(average_train_ols, average_test_ols, test_error_ols, average_train_nn, average_test_nn, test_error_nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T6LFT9xeFA8j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "9a13926d-4637-4246-fde0-1952d0858c3d"
      },
      "cell_type": "code",
      "source": [
        "## Repeat this cell varying number of observations and noise\n",
        "\n",
        "train_ols, test_ols, std_ols, train_nn, test_nn, std_nn = compare_regression_data1(observations = 1000, sigma = 0.01, test_size = 0.33, vector = [1024,64,1], learn = 0.01,epochs = 1000, rounds = 30)\n",
        "\n",
        "print(\"Training Error OLS: {}        |  Training Error NN: {}\".format(train_ols, train_nn))\n",
        "print(\"Testing Error OLS: {}         |  Testing Error NN: {}\".format(test_ols, test_nn))\n",
        "print(\"Testing STD ols: {}           |  Testing std NN: {}\".format(std_ols, std_nn))\n",
        "\n",
        "print(\"MSE difference between OLS and NN: {}\".format(test_ols - test_nn))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Error OLS: 0.010045809177736327        |  Training Error NN: 0.5876202702555848\n",
            "Testing Error OLS: 0.009969404608385501         |  Testing Error NN: 0.5894856288769801\n",
            "Testing STD ols: 0.00033595196902376924           |  Testing std NN: 0.02785584769492323\n",
            "MSE difference between OLS and NN: -0.5795162242685946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vnHXyBAxGbd1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NjIXk48-GcBh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RyQgCa-YMpz8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Regression - 2 continuous, one cat, linear"
      ]
    },
    {
      "metadata": {
        "id": "vacpKLm6Mtmb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data2(observations, sigma, representation):\n",
        " \n",
        "\n",
        "  x1 = np.random.randint(0,100, size = [observations*1]).reshape([observations,1])/10  ## Data points with n observations and m features\n",
        "  x2 = np.random.randint(0,50, size = [observations*1]).reshape([observations,1])/10  ## Data points with n observations and m feature\n",
        "   \n",
        "  x_cat = np.random.randint(7, size = [observations,1]).astype(int)  ## Add categorical variable (ranging between 1,7 to represent days of week)\n",
        "  \n",
        "  \n",
        "  ## Convert to dataframe to use \"get_dummies\" function\n",
        "  x = np.hstack((x1,x2,  x_cat))\n",
        "  x_original = pd.DataFrame(x)\n",
        "  x = pd.DataFrame(x)\n",
        "  cats = x.iloc[:,-1].astype(object)\n",
        "  cont = x.iloc[:,:-1]\n",
        "  cats = pd.get_dummies(cats, drop_first = True)\n",
        "   \n",
        "  x = pd.concat([cont,cats], axis = 1)\n",
        "  \n",
        "  a = np.array([[2,5,1,1,1,1,7,7]]) ## Set cooeficients manually\n",
        "  e = np.random.normal(0, sigma, size = [observations,1]).astype(float) ## This term adds 'noise' to the data\n",
        "  y = np.dot(x,a.T) + e\n",
        "  \n",
        "  ## Create dataframe's with one-hot encoding for OLS and without for NN\n",
        "  df1 = pd.DataFrame(x)\n",
        "  df2 = pd.DataFrame(y)\n",
        "  df_nn = pd.concat([x_original, df2], axis = 1)\n",
        "  df_ols = pd.concat([df1, df2], axis = 1)\n",
        "  \n",
        "  df_ols.columns = [\"x1\", \"x2\", \"cat1\",\"cat2\",\"cat3\",\"cat4\",\"cat5\",\"cat6\", \"y\"]\n",
        "  df_nn.columns = [\"x1\",\"x2\",\"cat\",\"y\"]\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  if representation == \"ols\":\n",
        "    return(df_ols)\n",
        "  \n",
        "  else:\n",
        "    return(df_nn)\n",
        "\n",
        "def split(df, test_size):\n",
        "  \n",
        "  test_size = int(np.round(test_size*df[\"y\"].size))\n",
        "  n = 1 - test_size\n",
        "  train = df.iloc[:n-1,:]\n",
        "  test = df.iloc[n-1:,:]\n",
        "  return(train, test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SrSpoEuuNVYU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compare_regression_data2(observations, sigma, test_size, vector, learn, output, epochs, rounds):\n",
        "  train_ols_list = []\n",
        "  test_ols_list = []\n",
        "  \n",
        "  \n",
        "  train_nn_list = []\n",
        "  test_nn_list = []\n",
        "  \n",
        "  #NN\n",
        "  df = get_data2(observations = observations, sigma = sigma, representation = \"nn\"  )\n",
        "  x = np.array(df.iloc[:, :-1])\n",
        "  y = np.array(df.iloc[:,-1])\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=test_size) ## Split data into training and testing set\n",
        "        \n",
        "  ## Normalise  \n",
        "  scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "  scalarX.fit(X_train[:,:-1])\n",
        "  \n",
        "  X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "  \n",
        "  X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "  \n",
        "  \n",
        "  # Define the embedding input\n",
        "  cat_input = Input(shape=(1,)) \n",
        "  \n",
        "  ## Numerical inut\n",
        "  num_input = Input(shape = (2,))\n",
        "  \n",
        "  ## Create embedding layer\n",
        "  embeding = Embedding(input_dim = 7\n",
        "                            , output_dim = output, input_length = 1)(cat_input)\n",
        "  \n",
        "  ## Flatten embedding layer\n",
        "  embeding = Flatten()(embeding)\n",
        "  \n",
        "  ## Concatenate numerical layer and embedding layer\n",
        "  combined = keras.layers.concatenate([num_input, embeding])\n",
        "  \n",
        "  ## Create normal dense layers\n",
        "  dense1 = Dense(units=vector[0], activation='relu')(combined)\n",
        "  dense2 = Dense(units = vector[1], activation='relu')(dense1)\n",
        "  predictions = Dense(1, activation = 'linear')(dense2)\n",
        "  \n",
        "  ## Define final model\n",
        "  model = Model(inputs=[num_input, cat_input], outputs=predictions)\n",
        "  \n",
        "  \n",
        "  optimizer = tf.train.AdamOptimizer(learn) \n",
        "    \n",
        "  model.compile(loss='mse',             #Set loss as MSE, and use adam optimizer\n",
        "                    optimizer=optimizer,\n",
        "                    metrics=['mse'])\n",
        "  \n",
        "  \n",
        " \n",
        "    \n",
        "  # Store training stats\n",
        "  history = model.fit([X_train[:,:-1], X_train[:,-1]],y_train, epochs=epochs,\n",
        "                    validation_split=0.2, verbose=0, callbacks = [EarlyStopping(monitor='val_mean_squared_error', patience=200)])\n",
        "  \n",
        "  \n",
        "  \n",
        "  [mse, acc] = model.evaluate([X_train[:,:-1], X_train[:,-1]],y_train, verbose=0)\n",
        "  \n",
        "  train_nn = sqrt(mse)\n",
        "  \n",
        "  test_nn_pred = np.round(model.predict([X_test[:,:-1], X_test[:,-1]]).astype(np.float64))\n",
        "  \n",
        "  test_nn = sqrt(metrics.mean_squared_error(y_test, test_nn_pred))\n",
        "      \n",
        "    \n",
        "  train_nn_list.append(train_nn)\n",
        "  test_nn_list.append(test_nn)\n",
        "  \n",
        "  ## Test on multiple sets\n",
        "  for i in range(rounds):\n",
        "    \n",
        "    ## OLS\n",
        "    df = get_data2(observations = observations, sigma = sigma, representation = \"ols\"  )\n",
        "\n",
        "    train, test = split(df, test_size = test_size)\n",
        "    \n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "    \n",
        "    x = train.iloc[:,:-1].astype(float)\n",
        "    y = train.iloc[:,-1]\n",
        "    scalarX.fit(x)\n",
        "    x = scalarX.transform(x)\n",
        "    \n",
        "    train = pd.concat([pd.DataFrame(x), y], axis = 1)\n",
        "    train.columns = [\"x1\", \"x2\", \"cat1\",\"cat2\",\"cat3\",\"cat4\",\"cat5\",\"cat6\", \"y\"]\n",
        "    \n",
        "    x1 = test.iloc[:,:-1].values\n",
        "    y1 = test.iloc[:,-1].values\n",
        "    \n",
        "    x1 = scalarX.transform(x1)\n",
        "    \n",
        "    test = pd.concat([pd.DataFrame(x1), pd.DataFrame(y1)], axis = 1)\n",
        "    test.columns = [\"x1\", \"x2\", \"cat1\",\"cat2\",\"cat3\",\"cat4\",\"cat5\",\"cat6\", \"y\"]\n",
        "\n",
        "    \n",
        "    results = smf.ols(formula = 'y ~ x1 + x2 + cat1 + cat2 +cat3 + cat4 + cat5 + cat6', data = sm.add_constant(train)).fit()\n",
        "    \n",
        "    train_pred_ols = results.fittedvalues\n",
        "    train_ols = sqrt(metrics.mean_squared_error(train_pred_ols, train[\"y\"]))\n",
        "    \n",
        "    test_pred_ols = results.predict(test)\n",
        "    test_ols = sqrt(metrics.mean_squared_error(test_pred_ols, test[\"y\"]))\n",
        "    \n",
        "    train_ols_list.append(train_ols)\n",
        "    test_ols_list.append(test_ols)\n",
        "    \n",
        "    \n",
        "    ## NN\n",
        "    df = get_data2(observations = observations, sigma = sigma, representation = \"nn\"  )\n",
        "\n",
        "    x = np.array(df.iloc[:, :-1])\n",
        "    y = np.array(df.iloc[:,-1])\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=test_size) ## Split data into training and testing set\n",
        "          \n",
        "    ## Normalise  \n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "    scalarX.fit(X_train[:,:-1])\n",
        "   \n",
        "    X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "    \n",
        "    X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    [mse, acc] = model.evaluate([X_train[:,:-1], X_train[:,-1]],y_train, verbose=0)\n",
        "    \n",
        "    train_nn = sqrt(mse)\n",
        "    \n",
        "    test_nn_pred = np.round(model.predict([X_test[:,:-1], X_test[:,-1]]).astype(np.float64))\n",
        "    \n",
        "    test_nn = sqrt(metrics.mean_squared_error(y_test, test_nn_pred))\n",
        "        \n",
        "      \n",
        "    train_nn_list.append(train_nn)\n",
        "    test_nn_list.append(test_nn)\n",
        "  \n",
        "  ## Find average MSE and standard deviation of MSE\n",
        "  average_train_ols = np.mean(train_ols_list)\n",
        "  average_test_ols = np.mean(test_ols_list)\n",
        "  std_ols = np.std(test_ols_list)                                  \n",
        "    \n",
        "  average_train_nn = np.mean(train_nn_list)\n",
        "  average_test_nn = np.mean(test_nn_list)\n",
        "  std_nn = np.std(test_nn_list)                                  \n",
        "                                    \n",
        "\n",
        "  return(average_train_ols, average_test_ols, std_ols,average_train_nn, average_test_nn, std_nn)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XDijZWzxN8j2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "13f4d6ba-3036-4592-b255-1bfacf3b80ef"
      },
      "cell_type": "code",
      "source": [
        "## Repeat this cell varying number of observations and noise\n",
        "\n",
        "train_ols, test_ols,std_ols, train_nn, test_nn, std_nn = compare_regression_data2(observations = 1000, sigma = 0.1, test_size = 0.33, vector = [1024,512,64,1], learn = 0.01,output = 4,epochs = 1000, rounds = 30)\n",
        "\n",
        "print(\"Training Error OLS: {}        |  Training Error NN: {}\".format(train_ols, train_nn))\n",
        "print(\"Testing Error OLS: {}         |  Testing Error NN: {}\".format(test_ols, test_nn))\n",
        "print(\"Testing STD ols: {}           |  Testing std NN: {}\".format(std_ols, std_nn))\n",
        "\n",
        "print(\"MSE difference between OLS and NN: {}\".format(test_ols - test_nn))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Error OLS: 0.099027481517117        |  Training Error NN: 0.2078731060828655\n",
            "Testing Error OLS: 0.10033605447653055         |  Testing Error NN: 0.3549832930734981\n",
            "Testing STD ols: 0.0027534829254504513           |  Testing std NN: 0.010533217009612052\n",
            "MSE difference between OLS and NN: -0.25464723859696753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wLRG96bTOA8i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Regression - 2 continuous, one cat, non-linear"
      ]
    },
    {
      "metadata": {
        "id": "8IoZRP06ODvx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data3(observations, sigma, representation):\n",
        " \n",
        "\n",
        "  x1 = np.random.randint(0,100, size = [observations*1]).reshape([observations,1])/10  ## Data points with n observations and m features\n",
        "  x2 = np.random.randint(0,50, size = [observations*1]).reshape([observations,1])/10  ## Data points with n observations and m feature\n",
        "   \n",
        "  x_cat = np.random.randint(7, size = [observations,1]).astype(int)  ## Add categorical variable (ranging between 1,7 to represent days of week)\n",
        "  \n",
        "  \n",
        "  ## Create dataframe and create one-hot encoding\n",
        "  x = np.hstack((x1,x2,  x_cat))\n",
        "  x_original = pd.DataFrame(x)\n",
        "  x = pd.DataFrame(x)\n",
        "  cats = x.iloc[:,-1].astype(object)\n",
        "  cont = x.iloc[:,:-1]\n",
        "  cats = pd.get_dummies(cats, drop_first = True)\n",
        "   \n",
        "  x = pd.concat([cont,cats], axis = 1)\n",
        "  x = np.array(x)\n",
        "  cats = np.array(cats)\n",
        "  \n",
        "  ## Set coefficients manually\n",
        "  a = np.array([[1,1,1,1,7,7]])\n",
        "  e = np.random.normal(0, sigma, size = [observations,1]).astype(float) ## This term adds 'noise' to the data\n",
        "  w1 = 2\n",
        "  w2 = 5\n",
        "  \n",
        "  ## Create non-linear relationship \n",
        "  y = np.dot(x1**2,w1) + np.dot(x2, w2) + np.dot(cats, a.T) + e\n",
        "  \n",
        "  df1 = pd.DataFrame(x)\n",
        "  df2 = pd.DataFrame(y)\n",
        "  df_nn = pd.concat([x_original, df2], axis = 1)\n",
        "  df_ols = pd.concat([df1, df2], axis = 1)\n",
        "  \n",
        "  \n",
        "  df_ols.columns = [\"x1\", \"x2\", \"cat1\",\"cat2\",\"cat3\",\"cat4\",\"cat5\",\"cat6\", \"y\"]\n",
        "  df_nn.columns = [\"x1\",\"x2\",\"cat\",\"y\"]\n",
        "  \n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  if representation == \"ols\":\n",
        "    return(df_ols)\n",
        "  \n",
        "  else:\n",
        "    return(df_nn)\n",
        "\n",
        "def split(df, test_size):\n",
        "  \n",
        "  test_size = int(np.round(test_size*df[\"y\"].size))\n",
        "  n = 1 - test_size\n",
        "  train = df.iloc[:n-1,:]\n",
        "  test = df.iloc[n-1:,:]\n",
        "  return(train, test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_wv26DMwOT-N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compare_regression_data3(observations, sigma, test_size, vector, learn, output, epochs, rounds):\n",
        "  train_ols_list = []\n",
        "  test_ols_list = []\n",
        "  \n",
        "  \n",
        "  train_nn_list = []\n",
        "  test_nn_list = []\n",
        "  \n",
        "  #NN\n",
        "  df = get_data3(observations = observations, sigma = sigma, representation = \"nn\"  )\n",
        "\n",
        "  x = np.array(df.iloc[:, :-1])\n",
        "  y = np.array(df.iloc[:,-1])\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=test_size) ## Split data into training and testing set\n",
        "        \n",
        "  scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "  scalarX.fit(X_train[:,:-1])\n",
        "  \n",
        "  X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "  \n",
        "  X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "  \n",
        "  \n",
        "  cat_input = Input(shape=(1,)) \n",
        "  \n",
        "  num_input = Input(shape = (2,))\n",
        "  \n",
        "  embeding = Embedding(input_dim = 7\n",
        "                            , output_dim = output, input_length = 1)(cat_input)\n",
        "  embeding = Flatten()(embeding)\n",
        "  \n",
        "  combined = keras.layers.concatenate([num_input, embeding])\n",
        "  \n",
        "  \n",
        "  \n",
        "  dense1 = Dense(units=vector[0], activation='relu')(combined)\n",
        "  dense2 = Dense(units = vector[1], activation='relu')(dense1)\n",
        "  predictions = Dense(1, activation = 'linear')(dense2)\n",
        "  \n",
        "  model = Model(inputs=[num_input, cat_input], outputs=predictions)\n",
        "  \n",
        "  \n",
        "  optimizer = tf.train.AdamOptimizer(learn) \n",
        "    \n",
        "  model.compile(loss='mse',             #Set loss as MSE, and use adam optimizer\n",
        "                    optimizer=optimizer,\n",
        "                    metrics=['mse'])\n",
        "  \n",
        "  \n",
        "  \n",
        "    \n",
        "  # Store training stats\n",
        "  history = model.fit([X_train[:,:-1], X_train[:,-1]],y_train, epochs=epochs,\n",
        "                    validation_split=0.2, verbose=0, callbacks = [EarlyStopping(monitor='val_mean_squared_error', patience=200)])\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  [mse, acc] = model.evaluate([X_train[:,:-1], X_train[:,-1]],y_train, verbose=0)\n",
        "  \n",
        "  train_nn = sqrt(mse)\n",
        "  \n",
        "  test_nn_pred = np.round(model.predict([X_test[:,:-1], X_test[:,-1]]).astype(np.float64))\n",
        "  \n",
        "  test_nn = sqrt(metrics.mean_squared_error(y_test, test_nn_pred))\n",
        "      \n",
        "    \n",
        "  train_nn_list.append(train_nn)\n",
        "  test_nn_list.append(test_nn)\n",
        "  \n",
        "  for i in range(rounds):\n",
        "    \n",
        "    ## OLS\n",
        "    df = get_data3(observations = observations, sigma = sigma, representation = \"ols\"  )\n",
        "    \n",
        "\n",
        "    train, test = split(df, test_size = test_size)\n",
        "    \n",
        "    \n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "    \n",
        "    x = train.iloc[:,:-1].astype(float)\n",
        "    y = train.iloc[:,-1]\n",
        "    scalarX.fit(x)\n",
        "    x = scalarX.transform(x)\n",
        "    \n",
        "    train = pd.concat([pd.DataFrame(x), y], axis = 1)\n",
        "    train.columns = [\"x1\", \"x2\", \"cat1\",\"cat2\",\"cat3\",\"cat4\",\"cat5\",\"cat6\", \"y\"]\n",
        "    \n",
        "    x1 = test.iloc[:,:-1].values\n",
        "    y1 = test.iloc[:,-1].values\n",
        "    \n",
        "    x1 = scalarX.transform(x1)\n",
        "    \n",
        "    test = pd.concat([pd.DataFrame(x1), pd.DataFrame(y1)], axis = 1)\n",
        "    test.columns = [\"x1\", \"x2\", \"cat1\",\"cat2\",\"cat3\",\"cat4\",\"cat5\",\"cat6\", \"y\"]\n",
        "    \n",
        "    results = smf.ols(formula = 'y ~ np.power(x1,2) + x2 + cat1 + cat2 +cat3 + cat4 + cat5 + cat6', data = sm.add_constant(train)).fit()\n",
        "    \n",
        "    train_pred_ols = results.fittedvalues\n",
        "    train_ols = sqrt(metrics.mean_squared_error(train_pred_ols, train[\"y\"]))\n",
        "    \n",
        "    test_pred_ols = results.predict(test)\n",
        "    test_ols = sqrt(metrics.mean_squared_error(test_pred_ols, test[\"y\"]))\n",
        "    \n",
        "    train_ols_list.append(train_ols)\n",
        "    test_ols_list.append(test_ols)\n",
        "    \n",
        "    \n",
        "    ##NN\n",
        "    df = get_data3(observations = observations, sigma = sigma, representation = \"nn\"  )\n",
        "\n",
        "    x = np.array(df.iloc[:, :-1])\n",
        "    y = np.array(df.iloc[:,-1])\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=test_size) ## Split data into training and testing set\n",
        "          \n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "    scalarX.fit(X_train[:,:-1])\n",
        "    \n",
        "    X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "    \n",
        "    X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "   \n",
        "    \n",
        "    [mse, acc] = model.evaluate([X_train[:,:-1], X_train[:,-1]],y_train, verbose=0)\n",
        "    \n",
        "    train_nn = sqrt(mse)\n",
        "    \n",
        "    test_nn_pred = np.round(model.predict([X_test[:,:-1], X_test[:,-1]]).astype(np.float64))\n",
        "    \n",
        "    test_nn = sqrt(metrics.mean_squared_error(y_test, test_nn_pred))\n",
        "        \n",
        "      \n",
        "    train_nn_list.append(train_nn)\n",
        "    test_nn_list.append(test_nn)\n",
        "  \n",
        "  \n",
        "  \n",
        "  average_train_ols = np.mean(train_ols_list)\n",
        "  average_test_ols = np.mean(test_ols_list)\n",
        "  std_ols = np.std(test_ols_list)                                  \n",
        "    \n",
        "  average_train_nn = np.mean(train_nn_list)\n",
        "  average_test_nn = np.mean(test_nn_list)\n",
        "  std_nn = np.std(test_nn_list)                                  \n",
        "                                    \n",
        "\n",
        "  return(average_train_ols, average_test_ols, std_ols,average_train_nn, average_test_nn, std_nn)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hQ5ORPjUOjRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "70dcc3da-4c51-4930-d124-9e6c83dd2020"
      },
      "cell_type": "code",
      "source": [
        "## Repeat this cell varying number of observations and noise\n",
        "\n",
        "train_ols, test_ols, std_ols, train_nn, std_nn, test_nn = compare_regression_data3(observations = 1000, sigma = 1, test_size = 0.33, vector = [1024,64,1], learn = 0.01,output = 4,epochs = 1000, rounds = 30)\n",
        "\n",
        "print(\"Training Error OLS: {}        |  Training Error NN: {}\".format(train_ols, train_nn))\n",
        "print(\"Testing Error OLS: {}         |  Testing Error NN: {}\".format(test_ols, test_nn))\n",
        "print(\"Testing STD ols: {}           |  Testing std NN: {}\".format(std_ols, std_nn))\n",
        "\n",
        "print(\"MSE difference between OLS and NN: {}\".format(test_ols - test_nn))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Error OLS: 0.9943417008746666        |  Training Error NN: 2.141814624905736\n",
            "Testing Error OLS: 1.0140024288054403         |  Testing Error NN: 0.09396211447858871\n",
            "Testing STD ols: 0.04275926834092917           |  Testing std NN: 2.176629027646439\n",
            "MSE difference between OLS and NN: 0.9200403143268516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rfhJ99fiOnZE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ]
    },
    {
      "metadata": {
        "id": "wmrTQeGyOwrC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classification, 2 continous"
      ]
    },
    {
      "metadata": {
        "id": "ZeubtW8NOpf-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_cat_data1(observations,  sigma):\n",
        "\n",
        "  ## Set coeeficients\n",
        "  x1 = np.random.randint(0,100, size = [observations,1]).astype(float)/10\n",
        "  x2 = np.random.randint(0,50, size = [observations,1]).astype(float)/10\n",
        "  w1 = 5\n",
        "  w2 = 2\n",
        "  \n",
        "  e = np.random.normal(0, sigma, size = [observations,1]).astype(float) ## This term adds 'noise' to the data\n",
        "  \n",
        "  x = np.hstack((x1,x2))\n",
        "  y = np.dot(x1**2,w1) + np.dot(x2,w2)  + e\n",
        "  \n",
        "  ## Convert output to binary\n",
        "  scaler = MinMaxScaler()\n",
        "  y = scaler.fit_transform(y) ## Scale y value\n",
        "  j = y    \n",
        "  L = [0 if i < j.mean() else 1 for i in y]  ## Split by mean so that both categories have equal representation\n",
        "  y = np.array(L).reshape(y.shape[0],1)\n",
        "  \n",
        "  df1 = pd.DataFrame(x)\n",
        "  df2 = pd.DataFrame(y)\n",
        "  df = pd.concat([df1, df2], axis = 1)\n",
        "  \n",
        "  df.columns = [\"x1\", \"x2\",  \"y\"]\n",
        "\n",
        "  return(df)\n",
        "\n",
        "def split(df, test_size):\n",
        "  \n",
        "  test_size = int(np.round(test_size*df[\"y\"].size))\n",
        "  n = 1 - test_size\n",
        "  train = df.iloc[:n-1,:]\n",
        "  test = df.iloc[n-1:,:]\n",
        "  return(train, test)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bVyNZ7RNO_Dp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compare_classification_data1(observations, sigma, test_size,  vector, learn,  epochs, rounds):\n",
        "  train_ols_list = []\n",
        "  test_ols_list = []\n",
        "  \n",
        "  \n",
        "  train_nn_list = []\n",
        "  test_nn_list = []\n",
        "  \n",
        "  def build_classification_model(vector, learn):\n",
        "     model = Sequential()\n",
        "     \n",
        "     \n",
        "     model.add(Dense(vector[0], activation = tf.nn.relu,\n",
        "                     input_dim=X_train.shape[1]))\n",
        "     \n",
        "     for i in range(1,len(vector)-1):\n",
        "       \n",
        "        model.add(Dense(vector[i], activation=tf.nn.relu))\n",
        "        \n",
        "         \n",
        "       \n",
        "     model.add(Dense(1, activation = tf.nn.sigmoid))\n",
        "     \n",
        "   \n",
        "     optimizer = tf.train.GradientDescentOptimizer(learn)\n",
        "     \n",
        "     ## Use binary crossentropy as loss function and accuracy as metric\n",
        "     model.compile(optimizer=tf.train.AdamOptimizer(), \n",
        "                 loss= 'binary_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "     return(model)\n",
        "  \n",
        " \n",
        "  df = get_cat_data1(observations = observations, sigma = sigma)\n",
        "  x = np.array(df.iloc[:, :-1])\n",
        "  y = np.array(df.iloc[:,-1])\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x,y, test_size= test_size) ## Split data into training and testing set\n",
        "  \n",
        "  ## Normalise data\n",
        "  scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "  scalarX.fit(X_train)\n",
        "  \n",
        "  X_train = scalarX.transform(X_train)\n",
        "  \n",
        "  X_test = scalarX.transform(X_test)\n",
        "  \n",
        "  model = build_classification_model(vector, learn)\n",
        "   \n",
        " \n",
        "  model.fit(X_train,y_train, epochs=epochs,\n",
        "                      validation_split=0.2, verbose=0)\n",
        "  \n",
        "  [loss, acc] = model.evaluate(X_train,y_train, verbose=0)\n",
        "  \n",
        "  \n",
        "  ## Round predictions to convert them to binary\n",
        "  test_nn_pred = np.round(model.predict(X_test))\n",
        "  \n",
        "  test_nn = metrics.accuracy_score(test_nn_pred, y_test)\n",
        "       \n",
        "     \n",
        "  train_nn_list.append(acc)\n",
        "  test_nn_list.append(test_nn)\n",
        "  \n",
        "  \n",
        "  for i in range(rounds):\n",
        "    \n",
        "    ## OLS\n",
        "    df = get_cat_data1(observations = observations, sigma = sigma)\n",
        "\n",
        "    train, test = split(df, test_size = test_size)\n",
        "\n",
        "    \n",
        "    ## Normalise\n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "    \n",
        "    x = train.iloc[:,:-1].astype(float)\n",
        "    y = train.iloc[:,-1]\n",
        "    scalarX.fit(x)\n",
        "    x = scalarX.transform(x)\n",
        "    \n",
        "    train = pd.concat([pd.DataFrame(x), y], axis = 1)\n",
        "    train.columns = [\"x1\", \"x2\",  \"y\"]\n",
        "    \n",
        "    x1 = test.iloc[:,:-1].values\n",
        "    y1 = test.iloc[:,-1].values\n",
        "    \n",
        "    x1 = scalarX.transform(x1)\n",
        "    \n",
        "    test = pd.concat([pd.DataFrame(x1), pd.DataFrame(y1)], axis = 1)\n",
        "    test.columns = [\"x1\", \"x2\",\"y\"]\n",
        "    \n",
        "    \n",
        "    results = smf.logit(formula = 'y ~ np.power(x1,2) + x2 ', data = sm.add_constant(train)).fit(maxiter = 200, method = 'bfgs', disp = False)\n",
        "    \n",
        "    train_pred_ols = (results.fittedvalues > 0.5).astype(int)\n",
        "        \n",
        "        \n",
        "    test_ols = results.predict(sm.add_constant(test))\n",
        "    \n",
        "    ## Round output to convert to binary\n",
        "    test_ols = (test_ols > 0.5).astype(int).values.reshape(test_ols.size,1)\n",
        "    \n",
        "    \n",
        "    train_ols = metrics.accuracy_score(train_pred_ols, train[\"y\"])\n",
        "    \n",
        "    \n",
        "    test_ols = metrics.accuracy_score(test_ols, test[\"y\"])\n",
        "\n",
        "    \n",
        "    train_ols_list.append(train_ols)\n",
        "    test_ols_list.append(test_ols)\n",
        "    \n",
        "\n",
        "\n",
        "    df = get_cat_data1(observations = observations, sigma = sigma)\n",
        "    x = np.array(df.iloc[:, :-1])\n",
        "    y = np.array(df.iloc[:,-1])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size= test_size) ## Split data into training and testing set\n",
        "    \n",
        "    ## Normalise data\n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "    scalarX.fit(X_train)\n",
        "    \n",
        "    X_train = scalarX.transform(X_train)\n",
        "    \n",
        "    X_test = scalarX.transform(X_test)\n",
        "   \n",
        "   \n",
        "    \n",
        "    [loss, acc] = model.evaluate(X_train,y_train, verbose=0)\n",
        "    \n",
        "    \n",
        "    ## Round ouput to convert it into binary\n",
        "    test_nn_pred = np.round(model.predict(X_test))\n",
        "    \n",
        "    test_nn = metrics.accuracy_score(test_nn_pred, y_test)\n",
        "\n",
        "        \n",
        "      \n",
        "    train_nn_list.append(acc)\n",
        "    test_nn_list.append(test_nn)\n",
        "  \n",
        "  \n",
        "  ## Return average accuracy and standard deviation of accuracy\n",
        "  average_train_ols = np.mean(train_ols_list)\n",
        "  average_test_ols = np.mean(test_ols_list)\n",
        "  std_ols = np.std(test_ols_list)\n",
        "    \n",
        "  average_train_nn = np.mean(train_nn_list)\n",
        "  average_test_nn = np.mean(test_nn_list)  \n",
        "  std_nn = np.std(test_nn_list)\n",
        "\n",
        "  return(average_train_ols, average_test_ols, std_ols, average_train_nn, average_test_nn, std_nn)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vg7xTtyYPZfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "d46599c4-2234-421a-fa9b-df4797bb10bd"
      },
      "cell_type": "code",
      "source": [
        "## Repeat this cell varying number of observations and noise\n",
        "\n",
        "train_ols, test_ols, std_ols, train_nn, test_nn, std_nn = compare_classification_data1(observations = 100, sigma = 0.01, test_size = 0.33,  vector = [1024,64,1], learn = 0.01, epochs = 1000, rounds= 30)\n",
        "print(\"Training Error OLS: {}        |  Training Error NN: {}\".format(train_ols, train_nn))\n",
        "print(\"Testing Error OLS: {}         |  Testing Error NN: {}\".format(test_ols, test_nn))\n",
        "print(\"Testing STD ols: {}           |  Testing std NN: {}\".format(std_ols, std_nn))\n",
        "\n",
        "print(\"MSE difference between OLS and NN: {}\".format(test_ols - test_nn))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Error OLS: 1.0        |  Training Error NN: 0.970149253903528\n",
            "Testing Error OLS: 0.9818181818181818         |  Testing Error NN: 0.9648093841642226\n",
            "Testing STD ols: 0.024242424242424235           |  Testing std NN: 0.03274317541435819\n",
            "MSE difference between OLS and NN: 0.01700879765395924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0-fkJGh5Pcmd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Classification, 2 continous, one categorical, linear **"
      ]
    },
    {
      "metadata": {
        "id": "7r36yQK_PhxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_cat_data2(observations, sigma, representation):\n",
        " \n",
        "\n",
        "  x1 = np.random.randint(0,100, size = [observations,1])/10  ## Data points with n observations and m features\n",
        "  x2 = np.random.randint(0,50, size = [observations,1])/10  ## Data points with n observations and m feature\n",
        "   \n",
        "  # Add categorical variable (ranging between 1,7 to represent days of week)  \n",
        "  x_cat = np.random.randint(7, size = [observations,1]).astype(int)  \n",
        "  \n",
        "  \n",
        "  ## Convert to dataframe and apply one-hot encoding\n",
        "  x = np.hstack((x1,x2,  x_cat))\n",
        "  x_original = pd.DataFrame(x)\n",
        "  x = pd.DataFrame(x)\n",
        "  cats = x.iloc[:,-1].astype(object)\n",
        "  cont = x.iloc[:,:-1]\n",
        "  cats = pd.get_dummies(cats, drop_first = True)\n",
        "   \n",
        "  x = pd.concat([cont,cats], axis = 1)\n",
        "  x = np.array(x)\n",
        "  \n",
        "  a = np.array([[2,5,1,1,1,1,7,7]])\n",
        "  e = np.random.normal(0, sigma, size = [observations,1]).astype(float) ## This term adds 'noise' to the data\n",
        "  \n",
        "  ## Linear relationship\n",
        "  y = np.dot(x,a.T) + e\n",
        "  \n",
        "  ## Convert output to binary, with equal number in each\n",
        "  scaler = MinMaxScaler()\n",
        "  y = scaler.fit_transform(y)\n",
        "  j = y\n",
        "    \n",
        "  L = [0 if i < j.mean() else 1 for i in y]\n",
        "    \n",
        "  y = np.array(L).reshape(y.shape[0],1)\n",
        "  \n",
        "  df1 = pd.DataFrame(x)\n",
        "  df2 = pd.DataFrame(y)\n",
        "  df_nn = pd.concat([x_original, df2], axis = 1)\n",
        "  df_ols = pd.concat([df1, df2], axis = 1)\n",
        "  \n",
        "  df_ols.columns = [\"x1\", \"x2\", \"cat1\",\"cat2\",\"cat3\",\"cat4\",\"cat5\",\"cat6\", \"y\"]\n",
        "  df_nn.columns = [\"x1\",\"x2\",\"cat\",\"y\"]\n",
        "  \n",
        "  \n",
        "  if representation == \"ols\":\n",
        "    return(df_ols)\n",
        "  \n",
        "  else:\n",
        "    return(df_nn)\n",
        "\n",
        "def split(df, test_size):\n",
        "  \n",
        "  test_size = int(np.round(test_size*df[\"y\"].size))\n",
        "  n = 1 - test_size\n",
        "  train = df.iloc[:n-1,:]\n",
        "  test = df.iloc[n-1:,:]\n",
        "  return(train, test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KlQv2GplP5_3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compare_classification_data2(observations, sigma, test_size,  vector, output, learn,  epochs, rounds):\n",
        "  train_ols_list = []\n",
        "  test_ols_list = []\n",
        "  \n",
        "  \n",
        "  train_nn_list = []\n",
        "  test_nn_list = []\n",
        "  \n",
        "  df= get_cat_data2(observations = observations, sigma = sigma, representation = \"nn\")\n",
        "  x = np.array(df.iloc[:, :-1])\n",
        "  y = np.array(df.iloc[:,-1])\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=test_size) ## Split data into training and testing set\n",
        "   \n",
        "  ## Normalise   \n",
        "  scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "  scalarX.fit(X_train[:,:-1])\n",
        "  \n",
        "  X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "  \n",
        "  X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "  \n",
        "  # Define the embedding input\n",
        "  cat_input = Input(shape=(1,), dtype='float32') \n",
        "  \n",
        "  ## Define numerical input\n",
        "  num_input = Input(shape = (2,), dtype = 'float32')\n",
        "  \n",
        "  ## Define embedding layer\n",
        "  embeding = Embedding(input_dim = 7\n",
        "                            , output_dim = output, input_length = 1 )(cat_input)\n",
        "  \n",
        "  ## Flatten embedding layer\n",
        "  embeding = Flatten()(embeding)\n",
        "  \n",
        "  ## Concatenate embedding with numerical\n",
        "  combined = keras.layers.concatenate([num_input, embeding])\n",
        "  \n",
        "  \n",
        "  ## Add dense layers\n",
        "  dense1 = Dense(units=vector[0], activation='relu')(combined)\n",
        "  dense2 = Dense(units = vector[1], activation='relu')(dense1)\n",
        "  \n",
        "  predictions = Dense(1, activation = 'sigmoid')(dense2)\n",
        "  \n",
        "  ## Define final model\n",
        "  model = Model(inputs=[num_input, cat_input], outputs=predictions)\n",
        "  \n",
        "  model.compile(optimizer=tf.train.AdamOptimizer(), \n",
        "            loss= 'binary_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  # Store training stats\n",
        "  history = model.fit([X_train[:,:-1], X_train[:,-1]],y_train, epochs=epochs,\n",
        "                    validation_split=0.2, verbose=0, callbacks = [EarlyStopping(monitor='val_acc', patience=150)])\n",
        "  \n",
        "  \n",
        "   \n",
        "  \n",
        "  \n",
        "  \n",
        "  [mse, acc] = model.evaluate([X_train[:,:-1], X_train[:,-1]],y_train, verbose=0)\n",
        "  \n",
        "  ## Round output to convert to binary\n",
        "  test_nn_pred = np.round(model.predict([X_test[:,:-1], X_test[:,-1]]).astype(np.float64))\n",
        "  \n",
        "       \n",
        "  \n",
        "  test_nn = metrics.accuracy_score(y_test, test_nn_pred)\n",
        "  train_nn_list.append(acc)\n",
        "  test_nn_list.append(test_nn)\n",
        "  \n",
        "  for i in range(rounds):\n",
        "    \n",
        "    ##OLS\n",
        "    df= get_cat_data2(observations = observations, sigma = sigma, representation = \"nn\")\n",
        "    train, test = split(df, test_size = test_size)\n",
        "    \n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "\n",
        "    x = train.iloc[:,:-1].astype(float)\n",
        "    y = train.iloc[:,-1]\n",
        "    scalarX.fit(x)\n",
        "    x = scalarX.transform(x)\n",
        "    \n",
        "    train = pd.concat([pd.DataFrame(x), y], axis = 1)\n",
        "    train.columns = [\"x1\", \"x2\", \"cat\", \"y\"]\n",
        "    \n",
        "    x1 = test.iloc[:,:-1].values\n",
        "    y1 = test.iloc[:,-1].values\n",
        "    \n",
        "    x1 = scalarX.transform(x1)\n",
        "    \n",
        "    test = pd.concat([pd.DataFrame(x1), pd.DataFrame(y1)], axis = 1)\n",
        "    test.columns = [\"x1\", \"x2\", \"cat\", \"y\"]\n",
        "    \n",
        "    results = smf.logit(formula = 'y ~ x1 + x2 + C(cat) ', data = sm.add_constant(train)).fit(method = 'bfgs', maxiter = 200, disp = False)\n",
        "    \n",
        "    train_pred_ols = (results.fittedvalues > 0.5).astype(int)\n",
        "        \n",
        "      \n",
        "        \n",
        "    test_ols = results.predict(sm.add_constant(test))\n",
        "    test_ols = (test_ols > 0.5).astype(int).values.reshape(test_ols.size,1)\n",
        "    \n",
        "    \n",
        "    train_ols = metrics.accuracy_score(train_pred_ols, train[\"y\"])\n",
        "    \n",
        "    \n",
        "    test_ols = metrics.accuracy_score(test_ols, test[\"y\"])\n",
        "        \n",
        "    train_ols_list.append(train_ols)\n",
        "    test_ols_list.append(test_ols)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    df= get_cat_data2(observations = observations, sigma = sigma, representation = \"nn\")\n",
        "    \n",
        "    x = np.array(df.iloc[:, :-1])\n",
        "    y = np.array(df.iloc[:,-1])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=test_size) ## Split data into training and testing set\n",
        "          \n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "    scalarX.fit(X_train[:,:-1])\n",
        "    \n",
        "    X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "    \n",
        "    X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "    \n",
        "\n",
        "    \n",
        "    [mse, acc] = model.evaluate([X_train[:,:-1], X_train[:,-1]],y_train, verbose=0)\n",
        "    \n",
        "    ## Round output to convert to binary\n",
        "    test_nn_pred = np.round(model.predict([X_test[:,:-1], X_test[:,-1]]).astype(np.float64))\n",
        "    \n",
        "         \n",
        "    \n",
        "    test_nn = metrics.accuracy_score(y_test, test_nn_pred)\n",
        "    train_nn_list.append(acc)\n",
        "    test_nn_list.append(test_nn)\n",
        "  \n",
        "  average_train_ols = np.mean(train_ols_list)\n",
        "  average_test_ols = np.mean(test_ols_list)\n",
        "  std_ols = np.std(test_ols_list)\n",
        "  \n",
        "  average_train_nn = np.mean(train_nn_list)\n",
        "  average_test_nn = np.mean(test_nn_list)\n",
        "  std_nn = np.std(test_nn_list)\n",
        "\n",
        "  return(average_train_ols, average_test_ols, std_ols, average_train_nn, average_test_nn, std_nn)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RqAnYXqLQyXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1795
        },
        "outputId": "acf08fac-5b83-4364-e48a-4f5e6149e35f"
      },
      "cell_type": "code",
      "source": [
        "## Repeat this cell varying number of observations and noise\n",
        "\n",
        "train_ols, test_ols, std_ols, train_nn, test_nn, std_nn = compare_classification_data2(observations = 200, sigma = 0.01, test_size = 0.33,  vector = [1024,64,1], output = 4, learn = 0.01,  epochs = 1000, rounds = 30)\n",
        "\n",
        "print(\"Training Error OLS: {}        |  Training Error NN: {}\".format(train_ols, train_nn))\n",
        "print(\"Testing Error OLS: {}         |  Testing Error NN: {}\".format(test_ols, test_nn))\n",
        "print(\"Testing STD ols: {}           |  Testing std NN: {}\".format(std_ols, std_nn))\n",
        "\n",
        "print(\"MSE difference between OLS and NN: {}\".format(test_ols - test_nn))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Error OLS: 1.0        |  Training Error NN: 0.9460760710844351\n",
            "Testing Error OLS: 0.9611111111111109         |  Testing Error NN: 0.9501466275659824\n",
            "Testing STD ols: 0.025558749351272718           |  Testing std NN: 0.03059331838617562\n",
            "MSE difference between OLS and NN: 0.010964483545128467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "jHiWrsoHQ3DS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classification, 2 continous, one categorical, non-linear"
      ]
    },
    {
      "metadata": {
        "id": "45h6__PDQ6X8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_cat_data3(observations, sigma,  representation):\n",
        " \n",
        "\n",
        "  x1 = np.random.randint(0,100, size = [observations*1]).reshape([observations,1])/10  ## Data points with n observations and m features\n",
        "  x2 = np.random.randint(0,50, size = [observations*1]).reshape([observations,1])/10  ## Data points with n observations and m feature\n",
        "   \n",
        "  x_cat = np.random.randint(7, size = [observations,1]).astype(int)  ## Add categorical variable (ranging between 1,7 to represent days of week)\n",
        "  \n",
        "  \n",
        "  \n",
        "  x = np.hstack((x1,x2,  x_cat))\n",
        "  x_original = pd.DataFrame(x)\n",
        "  x = pd.DataFrame(x)\n",
        "  cats = x.iloc[:,-1].astype(object)\n",
        "  cont = x.iloc[:,:-1]\n",
        "  cats = pd.get_dummies(cats, drop_first = True)\n",
        "   \n",
        "  x = pd.concat([cont,cats], axis = 1)\n",
        "  x = np.array(x)\n",
        "  cats = np.array(cats)\n",
        "  \n",
        "  a = np.array([[1,1,1,1,7,7]])\n",
        "  e = np.random.normal(0, sigma, size = [observations,1]).astype(float) ## This term adds 'noise' to the data\n",
        "  w1 = 2\n",
        "  w2 = 5\n",
        "  \n",
        "  ## Create non-linear relationship\n",
        "  y = np.dot(x1**2,w1) + np.dot(x2, w2) + np.dot(cats, a.T) + e\n",
        "  \n",
        "  scaler = MinMaxScaler()\n",
        "  y = scaler.fit_transform(y)\n",
        "  j = y\n",
        "    \n",
        "  L = [0 if i < j.mean() else 1 for i in y]\n",
        "    \n",
        "  y = np.array(L).reshape(y.shape[0],1)\n",
        "  \n",
        "  df1 = pd.DataFrame(x)\n",
        "  df2 = pd.DataFrame(y)\n",
        "  df_nn = pd.concat([x_original, df2], axis = 1)\n",
        "  df_ols = pd.concat([df1, df2], axis = 1)\n",
        "  \n",
        "  \n",
        "  df_ols.columns = [\"x1\", \"x2\", \"cat1\",\"cat2\",\"cat3\",\"cat4\",\"cat5\",\"cat6\", \"y\"]\n",
        "  df_nn.columns = [\"x1\",\"x2\",\"cat\",\"y\"]\n",
        "  \n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  if representation == \"ols\":\n",
        "    return(df_ols)\n",
        "  \n",
        "  else:\n",
        "    return(df_nn)\n",
        "\n",
        "def split(df, test_size):\n",
        "  \n",
        "  test_size = int(np.round(test_size*df[\"y\"].size))\n",
        "  n = 1 - test_size\n",
        "  train = df.iloc[:n-1,:]\n",
        "  test = df.iloc[n-1:,:]\n",
        "  return(train, test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "49rI3CPmRCbc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compare_classification_data3(observations, sigma, test_size,  vector, output, learn,  epochs, rounds):\n",
        "  train_ols_list = []\n",
        "  test_ols_list = []\n",
        "  \n",
        "  \n",
        "  train_nn_list = []\n",
        "  test_nn_list = []\n",
        "  \n",
        "  df= get_cat_data3(observations = observations, sigma = sigma, representation = \"nn\")\n",
        "    \n",
        "  x = np.array(df.iloc[:, :-1])\n",
        "  y = np.array(df.iloc[:,-1])\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=test_size) ## Split data into training and testing set\n",
        "  \n",
        "  ## Normalise\n",
        "  scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "  scalarX.fit(X_train[:,:-1])\n",
        "  \n",
        "  X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "  \n",
        "  X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "  \n",
        "  # Define the embedding input\n",
        "  cat_input = Input(shape=(1,), dtype='float32') \n",
        "  \n",
        "  ## Define numerical input\n",
        "  num_input = Input(shape = (2,), dtype = 'float32')\n",
        "  \n",
        "  ## Create embedding layer\n",
        "  embeding = Embedding(input_dim = 7\n",
        "                            , output_dim = output, input_length = 1 )(cat_input)\n",
        "  embeding = Flatten()(embeding)\n",
        "  \n",
        "  combined = keras.layers.concatenate([num_input, embeding])\n",
        "  \n",
        "  ## Create normal layers\n",
        "  \n",
        "  dense1 = Dense(units=vector[0], activation='relu')(combined)\n",
        "  dense2 = Dense(units = vector[1], activation='relu')(dense1)\n",
        "  predictions = Dense(1, activation = 'sigmoid')(dense2)\n",
        "  \n",
        "  ## Define final model\n",
        "  model = Model(inputs=[num_input, cat_input], outputs=predictions)\n",
        "  model.compile(optimizer=tf.train.AdamOptimizer(), \n",
        "            loss= 'binary_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "  \n",
        "  \n",
        "  \n",
        "  # Store training stats\n",
        "  history = model.fit([X_train[:,:-1], X_train[:,-1]],y_train, epochs=epochs,\n",
        "                    validation_split=0.2, verbose=0, callbacks = [EarlyStopping(monitor='val_acc', patience=150)])\n",
        "  \n",
        "  \n",
        "\n",
        "  \n",
        "  [mse, acc] = model.evaluate([X_train[:,:-1], X_train[:,-1]],y_train, verbose=0)\n",
        "    \n",
        "    \n",
        "  test_nn_pred = np.round(model.predict([X_test[:,:-1], X_test[:,-1]]).astype(np.float64))\n",
        "  \n",
        "  \n",
        "       \n",
        "  \n",
        "  test_nn = metrics.accuracy_score(y_test, test_nn_pred)\n",
        "  train_nn_list.append(acc)\n",
        "  test_nn_list.append(test_nn)\n",
        "  \n",
        "  for i in range(rounds):\n",
        "    \n",
        "    ## OLS\n",
        "    df = get_cat_data3(observations = observations, sigma = sigma,  representation = \"nn\")\n",
        "    train, test = split(df, test_size = test_size)\n",
        "    \n",
        "   \n",
        "    ## Normalise\n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "\n",
        "    x = train.iloc[:,:-1].astype(float)\n",
        "    y = train.iloc[:,-1]\n",
        "    scalarX.fit(x)\n",
        "    x = scalarX.transform(x)\n",
        "    \n",
        "    train = pd.concat([pd.DataFrame(x), y], axis = 1)\n",
        "    train.columns = [\"x1\", \"x2\", \"cat\", \"y\"]\n",
        "    \n",
        "    x1 = test.iloc[:,:-1].values\n",
        "    y1 = test.iloc[:,-1].values\n",
        "    \n",
        "    x1 = scalarX.transform(x1)\n",
        "    \n",
        "    test = pd.concat([pd.DataFrame(x1), pd.DataFrame(y1)], axis = 1)\n",
        "    test.columns = [\"x1\", \"x2\", \"cat\", \"y\"]\n",
        "    \n",
        "    results = smf.logit(formula = 'y ~ np.power(x1,2) + x2 + C(cat) ', data = sm.add_constant(train)).fit(method = 'bfgs', maxiter = 200,disp = False)\n",
        "    \n",
        "    train_pred_ols = (results.fittedvalues > 0.5).astype(int)\n",
        "        \n",
        "        \n",
        "    test_ols = results.predict(sm.add_constant(test))\n",
        "    test_ols = (test_ols > 0.5).astype(int).values.reshape(test_ols.size,1)\n",
        "    \n",
        "    \n",
        "    train_ols = metrics.accuracy_score(train_pred_ols, train[\"y\"])\n",
        "    \n",
        "    \n",
        "    test_ols = metrics.accuracy_score(test_ols, test[\"y\"])\n",
        "            \n",
        "    train_ols_list.append(train_ols)\n",
        "    test_ols_list.append(test_ols)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    df= get_cat_data3(observations = observations, sigma = sigma, representation = \"nn\")\n",
        "    \n",
        "    x = np.array(df.iloc[:, :-1])\n",
        "    y = np.array(df.iloc[:,-1])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=test_size) ## Split data into training and testing set\n",
        "    \n",
        "    ## Normalise\n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "    scalarX.fit(X_train[:,:-1])\n",
        "    \n",
        "    X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "    \n",
        "    X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "    \n",
        "\n",
        "    \n",
        "    [mse, acc] = model.evaluate([X_train[:,:-1], X_train[:,-1]],y_train, verbose=0)\n",
        "      \n",
        "      \n",
        "    test_nn_pred = np.round(model.predict([X_test[:,:-1], X_test[:,-1]]).astype(np.float64))\n",
        "\n",
        "    \n",
        "    \n",
        "    test_nn = metrics.accuracy_score(y_test, test_nn_pred)\n",
        "    train_nn_list.append(acc)\n",
        "    test_nn_list.append(test_nn)\n",
        "  \n",
        "  average_train_ols = np.mean(train_ols_list)\n",
        "  average_test_ols = np.mean(test_ols_list)\n",
        "  std_ols = np.std(test_ols_list)                        \n",
        "    \n",
        "  average_train_nn = np.mean(train_nn_list)\n",
        "  average_test_nn = np.mean(test_nn_list)\n",
        "  std_nn = np.std(test_nn_list)\n",
        "\n",
        "  return(average_train_ols, average_test_ols,std_ols, average_train_nn, average_test_nn, std_nn)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lXg1GSyjRKQr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1617
        },
        "outputId": "410300f1-c4e0-4654-e050-59337ebd0ecc"
      },
      "cell_type": "code",
      "source": [
        "## Repeat this cell varying number of observations and noise\n",
        "\n",
        "train_ols, test_ols,std_ols, train_nn, test_nn, std_nn = compare_classification_data3(observations = 1000, sigma = 0.1, test_size = 0.33,  vector = [1024,64,1], output = 4, learn = 0.01,  epochs = 1000, rounds = 30)\n",
        "\n",
        "print(\"Training Error OLS: {}        |  Training Error NN: {}\".format(train_ols, train_nn))\n",
        "print(\"Testing Error OLS: {}         |  Testing Error NN: {}\".format(test_ols, test_nn))\n",
        "print(\"Testing STD ols: {}           |  Testing std NN: {}\".format(std_ols, std_nn))\n",
        "\n",
        "print(\"MSE difference between OLS and NN: {}\".format(test_ols - test_nn))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
            "  'available', HessianInversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-X))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Error OLS: 1.0        |  Training Error NN: 0.9864708712024071\n",
            "Testing Error OLS: 0.9932323232323231         |  Testing Error NN: 0.9866080156402739\n",
            "Testing STD ols: 0.005679067796715385           |  Testing std NN: 0.00802755236271664\n",
            "MSE difference between OLS and NN: 0.006624307592049239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iW2jkQBURft_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# No Embedding"
      ]
    },
    {
      "metadata": {
        "id": "-dS8HpE2Rptr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Regression"
      ]
    },
    {
      "metadata": {
        "id": "zCSRnN8ORsWQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Regression, data2"
      ]
    },
    {
      "metadata": {
        "id": "GndX1TJiRhFP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def NN_regression_2(observations, sigma, vector,   rounds):\n",
        "\n",
        "  train = []\n",
        "  test = []\n",
        "  \n",
        "  vector = vector\n",
        "    \n",
        "  learn = 0.01\n",
        "   \n",
        "  \n",
        "  \n",
        "  \n",
        "  df = get_data2(observations = observations, sigma = sigma, representation = \"ols\"  )\n",
        "  \n",
        "  \n",
        "  x = np.array(df.iloc[:, :-1])\n",
        "  y = np.array(df.iloc[:,-1])\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.33) ## Split data into training and testing set\n",
        "    \n",
        "  ## Normalise  \n",
        "  scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "  scalarX.fit(X_train[:,:-1])\n",
        "\n",
        "  X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "\n",
        "  X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "  \n",
        "  \n",
        "  def build__regression_model(vector, learn):  ## Allow us to specify the size and number of neurons in the network with 'vector'\n",
        "      \n",
        "      model = Sequential()\n",
        "      \n",
        "      model.add(Dense(vector[0],   ## Fix input layer with relu activation function. Number of input neurons = number of features in the data\n",
        "                      input_dim=X_train.shape[1]) )\n",
        "      \n",
        "      for i in range(1,len(vector)-1):  ## Create loop which adds new layers of the sizes specified in 'vector'\n",
        "        \n",
        "         model.add(Dense(vector[i], activation=tf.nn.relu))\n",
        "         \n",
        "        \n",
        "      model.add(Dense(1, activation = \"linear\")) ## Set final layer with output dim = 1, and linear activation function\n",
        "      \n",
        "    \n",
        "      optimizer = tf.train.AdamOptimizer(learn) \n",
        "    \n",
        "      model.compile(loss='mse',             #Set loss as MSE, and use adam optimizer\n",
        "                    optimizer=optimizer,\n",
        "                    metrics=['mse'])\n",
        "      return model\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  model = build__regression_model(vector = vector, learn = learn)\n",
        "  EPOCHS = 1000\n",
        "  \n",
        "  # Store training stats\n",
        "  history = model.fit(X_train,y_train, epochs=EPOCHS,\n",
        "                    validation_split=0.2, verbose=0)\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  [mse, acc] = model.evaluate(X_train,y_train, verbose=0)\n",
        "  y_pred_nn = model.predict(X_test).astype(np.float64)\n",
        "  \n",
        "  \n",
        "  train_nn = sqrt(mse)\n",
        "  \n",
        "  train.append(train_nn)\n",
        "  \n",
        "  test_nn = sqrt(metrics.mean_squared_error(y_test, y_pred_nn))\n",
        "  test.append(test_nn)\n",
        "  for i in range(rounds):\n",
        "    \n",
        "    \n",
        "    \n",
        "    df = get_data2(observations = observations, sigma = sigma, representation = \"ols\"  )\n",
        "    \n",
        "    \n",
        "    x = np.array(df.iloc[:, :-1])\n",
        "    y = np.array(df.iloc[:,-1])\n",
        "    #\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.33) ## Split data into training and testing set\n",
        "    \n",
        "    ## Normalise\n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "    scalarX.fit(X_train[:,:-1])\n",
        "\n",
        "    X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        " \n",
        "    X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    [mse, acc] = model.evaluate(X_train,y_train, verbose=0)\n",
        "    y_pred_nn = model.predict(X_test).astype(np.float64)\n",
        "    \n",
        "    \n",
        "    train_nn = sqrt(mse)\n",
        "    \n",
        "    train.append(train_nn)\n",
        "    \n",
        "    test_nn = sqrt(metrics.mean_squared_error(y_test, y_pred_nn))\n",
        "    test.append(test_nn)\n",
        "    \n",
        "  average_train = np.mean(train)\n",
        "  average_test = np.mean(test)\n",
        "  std = np.std(test)\n",
        "  return(average_train, average_test, std)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-RGEzIxRR177",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d415622e-6f25-4803-ae89-be47ad0ea258"
      },
      "cell_type": "code",
      "source": [
        "## Repeat this cell varying number of observations and noise\n",
        "\n",
        "train, test, std = NN_regression_2(observations = 1000, sigma = 0.01, vector = [1024,64,1], rounds = 30)\n",
        "\n",
        "print(\"Train: {}  |  Test: {}     | std: {}\".format(train, test, std))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 0.07373493671151474  |  Test: 0.07303772513676275     | std: 0.0025677670291199676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_EsLOgjYR5Dh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Regression, data3"
      ]
    },
    {
      "metadata": {
        "id": "1VKVc0hoR84q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def NN_regression_3(observations, sigma, vector,   rounds):\n",
        "\n",
        "  train = []\n",
        "  test = []\n",
        "  \n",
        "  vector = vector\n",
        "  \n",
        "  learn = 0.01\n",
        "   \n",
        "  \n",
        "  \n",
        "  \n",
        "  df = get_data3(observations = observations, sigma = sigma, representation = \"ols\"  )\n",
        "  \n",
        "  \n",
        "  x = np.array(df.iloc[:, :-1])\n",
        "  y = np.array(df.iloc[:,-1])\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.33) ## Split data into training and testing set\n",
        "  \n",
        "  ## Normalise\n",
        "  scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "  scalarX.fit(X_train[:,:-1])\n",
        "  \n",
        "  X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "  \n",
        "  X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "  \n",
        "  \n",
        "  def build__regression_model(vector, learn):  ## Allow us to specify the size and number of neurons in the network with 'vector'\n",
        "      \n",
        "      model = Sequential()\n",
        "      \n",
        "      model.add(Dense(vector[0],   ## Fix input layer with relu activation function. Number of input neurons = number of features in the data\n",
        "                      input_dim=X_train.shape[1]) )\n",
        "      \n",
        "      for i in range(1,len(vector)-1):  ## Create loop which adds new layers of the sizes specified in 'vector'\n",
        "        \n",
        "         model.add(Dense(vector[i], activation=tf.nn.relu))\n",
        "         \n",
        "        \n",
        "      model.add(Dense(1, activation = \"linear\")) ## Set final layer with output dim = 1, and linear activation function\n",
        "      \n",
        "    \n",
        "      optimizer = tf.train.AdamOptimizer(learn) \n",
        "    \n",
        "      model.compile(loss='mse',             #Set loss as MSE, and use adam optimizer\n",
        "                    optimizer=optimizer,\n",
        "                    metrics=['mse'])\n",
        "      return model\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  model = build__regression_model(vector = vector, learn = learn)\n",
        "  EPOCHS = 1000\n",
        "  \n",
        "  # Store training stats\n",
        "  history = model.fit(X_train,y_train, epochs=EPOCHS,\n",
        "                    validation_split=0.2, verbose=0)\n",
        "  \n",
        "  \n",
        "   \n",
        "  \n",
        "  [mse, acc] = model.evaluate(X_train,y_train, verbose=0)\n",
        "  y_pred_nn = model.predict(X_test).astype(np.float64)\n",
        "  \n",
        "  \n",
        "  train_nn = sqrt(mse)\n",
        "  \n",
        "  train.append(train_nn)\n",
        "  \n",
        "  test_nn = sqrt(metrics.mean_squared_error(y_test, y_pred_nn))\n",
        "  test.append(test_nn)\n",
        "  for i in range(rounds):\n",
        "   \n",
        "   \n",
        "    \n",
        "    \n",
        "    df = get_data3(observations = observations, sigma = sigma, representation = \"ols\"  )\n",
        "    \n",
        "    \n",
        "    x = np.array(df.iloc[:, :-1])\n",
        "    y = np.array(df.iloc[:,-1])\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.33) ## Split data into training and testing set\n",
        "     \n",
        "    # Normalise  \n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "    scalarX.fit(X_train[:,:-1])\n",
        "   \n",
        "    X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "  \n",
        "    X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    [mse, acc] = model.evaluate(X_train,y_train, verbose=0)\n",
        "    y_pred_nn = model.predict(X_test).astype(np.float64)\n",
        "    \n",
        "    \n",
        "    train_nn = sqrt(mse)\n",
        "    \n",
        "    train.append(train_nn)\n",
        "    \n",
        "    test_nn = sqrt(metrics.mean_squared_error(y_test, y_pred_nn))\n",
        "    test.append(test_nn)\n",
        "    \n",
        "    \n",
        "  average_train = np.mean(train)\n",
        "  average_test = np.mean(test)\n",
        "  std = np.std(test)\n",
        "  return(average_train, average_test, std)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SBzpz3xCSHFL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fb1b03f5-ab2c-424b-d807-33d16e800b7e"
      },
      "cell_type": "code",
      "source": [
        "## Repeat this cell varying number of observations and noise\n",
        "train, test, std = NN_regression_3(observations = 1000, sigma = 0.01, vector = [1024,64,1], rounds = 30)\n",
        "\n",
        "print(\"Train: {}  |  Test: {}     | std: {}\".format(train, test, std))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 0.7554924980650136  |  Test: 0.7742341695003457     | std: 0.03629413108886143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zPQ6BNeySKPF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ]
    },
    {
      "metadata": {
        "id": "LGZGTjzQSN2k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Classification data2"
      ]
    },
    {
      "metadata": {
        "id": "vkPfRn4DSNTD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def NN_classification_2(observations, sigma, vector,   rounds):\n",
        "\n",
        "  train = []\n",
        "  test = []\n",
        "  \n",
        "  vector = vector\n",
        "    \n",
        "  learn = 0.01\n",
        "   \n",
        "  \n",
        "  \n",
        "  \n",
        "  df = get_cat_data2(observations = observations, sigma = sigma, representation = \"ols\"  )\n",
        "  \n",
        "  \n",
        "  x = np.array(df.iloc[:, :-1])\n",
        "  y = np.array(df.iloc[:,-1])\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.33) ## Split data into training and testing set\n",
        "   \n",
        "  ## Normalise  \n",
        "  scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "  scalarX.fit(X_train[:,:-1])\n",
        "  \n",
        "  X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "  \n",
        "  X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "  \n",
        "  \n",
        "  def build__classification_model(vector, learn):  ## Allow us to specify the size and number of neurons in the network with 'vector'\n",
        "      \n",
        "      model = Sequential()\n",
        "      \n",
        "      model.add(Dense(vector[0],   ## Fix input layer with relu activation function. Number of input neurons = number of features in the data\n",
        "                      input_dim=X_train.shape[1]) )\n",
        "      \n",
        "      for i in range(1,len(vector)-1):  ## Create loop which adds new layers of the sizes specified in 'vector'\n",
        "        \n",
        "         model.add(Dense(vector[i], activation=tf.nn.relu))\n",
        "         \n",
        "        \n",
        "      model.add(Dense(1, activation = \"sigmoid\")) ## Set final layer with output dim = 1, and linear activation function\n",
        "      \n",
        "    \n",
        "      optimizer = tf.train.AdamOptimizer(learn) \n",
        "    \n",
        "      model.compile(loss='binary_crossentropy',             #Set loss as MSE, and use adam optimizer\n",
        "                    optimizer=optimizer,\n",
        "                    metrics=['accuracy'])\n",
        "      return model\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  model = build__classification_model(vector = vector, learn = learn)\n",
        "  EPOCHS = 1000\n",
        "  \n",
        "  # Store training stats\n",
        "  history = model.fit(X_train,y_train, epochs=EPOCHS,\n",
        "                    validation_split=0.2, verbose=0)\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  [loss, acc] = model.evaluate(X_train,y_train, verbose=0)\n",
        "  test_nn_pred = np.round(model.predict(X_test).astype(np.float64))\n",
        "  \n",
        "  test_nn = metrics.accuracy_score(y_test, test_nn_pred)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  train.append(acc)\n",
        "  \n",
        "  \n",
        "  test.append(test_nn)\n",
        "  for i in range(rounds):\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    df = get_cat_data2(observations = observations, sigma = sigma, representation = \"ols\"  )\n",
        "    \n",
        "    \n",
        "    x = np.array(df.iloc[:, :-1])\n",
        "    y = np.array(df.iloc[:,-1])\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.33) ## Split data into training and testing set\n",
        "          \n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "    scalarX.fit(X_train[:,:-1])\n",
        "    \n",
        "    X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "    \n",
        "    X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "    \n",
        "    \n",
        "    \n",
        "   \n",
        "    \n",
        "    [loss, acc] = model.evaluate(X_train,y_train, verbose=0)\n",
        "\n",
        "\n",
        "    test_nn_pred = np.round(model.predict(X_test).astype(np.float64))\n",
        "\n",
        "     \n",
        "\n",
        "    test_nn = metrics.accuracy_score(y_test, test_nn_pred)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    train.append(acc)\n",
        "    \n",
        "    \n",
        "    test.append(test_nn)\n",
        "    \n",
        "  average_train = np.mean(train)\n",
        "  average_test = np.mean(test)\n",
        "  std = np.std(test)                        \n",
        "  return(average_train, average_test, std)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XqGVIZdaSXCK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1df8e4c1-279e-4a74-a9c0-867647659d94"
      },
      "cell_type": "code",
      "source": [
        "## Repeat this cell varying number of observations and noise\n",
        "train, test, std = NN_classification_2(observations = 200, sigma = 0.01, vector = [1024,64,1], rounds = 30)\n",
        "\n",
        "print(\"Train: {}  |  Test: {}     | std: {}\".format(train, test, std))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 0.9576311986436049  |  Test: 0.9516129032258065     | std: 0.026779330462957433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XVxE2liuSZZ4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Classification data3"
      ]
    },
    {
      "metadata": {
        "id": "Z-hRHaP_SbiF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def NN_classification_3(observations, sigma, vector,   rounds):\n",
        "\n",
        "  train = []\n",
        "  test = []\n",
        "  \n",
        "  vector = vector\n",
        "    \n",
        "  learn = 0.01\n",
        "   \n",
        "  \n",
        "  \n",
        "  \n",
        "  df = get_cat_data3(observations = observations, sigma = sigma, representation = \"ols\"  )\n",
        "  \n",
        "  \n",
        "  x = np.array(df.iloc[:, :-1])\n",
        "  y = np.array(df.iloc[:,-1])\n",
        "  #\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.33) ## Split data into training and testing set\n",
        "  \n",
        "  ## Normalise\n",
        "  scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "  scalarX.fit(X_train[:,:-1])\n",
        "\n",
        "  X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "\n",
        "  X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "  \n",
        "  \n",
        "  def build__classification_model(vector, learn):  ## Allow us to specify the size and number of neurons in the network with 'vector'\n",
        "      \n",
        "      model = Sequential()\n",
        "      \n",
        "      model.add(Dense(vector[0],   ## Fix input layer with relu activation function. Number of input neurons = number of features in the data\n",
        "                      input_dim=X_train.shape[1]) )\n",
        "      \n",
        "      for i in range(1,len(vector)-1):  ## Create loop which adds new layers of the sizes specified in 'vector'\n",
        "        \n",
        "         model.add(Dense(vector[i], activation=tf.nn.relu))\n",
        "         \n",
        "        \n",
        "      model.add(Dense(1, activation = \"sigmoid\")) ## Set final layer with output dim = 1, and linear activation function\n",
        "      \n",
        "    \n",
        "      optimizer = tf.train.AdamOptimizer(learn) \n",
        "    \n",
        "      model.compile(loss='binary_crossentropy',             #Set loss as MSE, and use adam optimizer\n",
        "                    optimizer=optimizer,\n",
        "                    metrics=['accuracy'])\n",
        "      return model\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  model = build__classification_model(vector = vector, learn = learn)\n",
        "  EPOCHS = 1000\n",
        "  \n",
        "  # Store training stats\n",
        "  history = model.fit(X_train,y_train, epochs=EPOCHS,\n",
        "                    validation_split=0.2, verbose=0)\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  [loss, acc] = model.evaluate(X_train,y_train, verbose=0)\n",
        "  test_nn_pred = np.round(model.predict(X_test).astype(np.float64))\n",
        "  \n",
        "  test_nn = metrics.accuracy_score(y_test, test_nn_pred)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  train.append(acc)\n",
        "  \n",
        "  \n",
        "  test.append(test_nn)\n",
        "    \n",
        "  for i in range(rounds):\n",
        "   \n",
        "    \n",
        "    \n",
        "    df = get_cat_data3(observations = observations, sigma = sigma, representation = \"ols\"  )\n",
        "    \n",
        "    \n",
        "    x = np.array(df.iloc[:, :-1])\n",
        "    y = np.array(df.iloc[:,-1])\n",
        "    #\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.33) ## Split data into training and testing set\n",
        "          \n",
        "    scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "    scalarX.fit(X_train[:,:-1])\n",
        "  \n",
        "    X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "    \n",
        "    X_test[:,:-1] = scalarX.transform(X_test[:,:-1])\n",
        "    \n",
        "    \n",
        "   \n",
        "    [loss, acc] = model.evaluate(X_train,y_train, verbose=0)\n",
        "\n",
        "\n",
        "    test_nn_pred = np.round(model.predict(X_test).astype(np.float64))\n",
        "\n",
        "     \n",
        "\n",
        "    test_nn = metrics.accuracy_score(y_test, test_nn_pred)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    train.append(acc)\n",
        "    \n",
        "    \n",
        "    test.append(test_nn)\n",
        "    \n",
        "  average_train = np.mean(train)\n",
        "  average_test = np.mean(test)\n",
        "  std = np.std(test)                        \n",
        "  return(average_train, average_test,std)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qo31bzCpSg-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "05a79f48-7241-4ccb-96ff-0b98d417603f"
      },
      "cell_type": "code",
      "source": [
        "## Repeat this cell varying number of observations and noise\n",
        "train, test, std = NN_classification_3(observations = 200, sigma = 0.01, vector = [1024,64,1], rounds = 30)\n",
        "\n",
        "print(\"Train: {}  |  Test: {}     | std: {}\".format(train, test, std))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f253a2c19a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN_classification_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train: {}  |  Test: {}     | std: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NN_classification_3' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "mhU3M-xMklbi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = get_data2(observations = 100, sigma = 0.01, representation = \"ols\"  )\n",
        "\n",
        "train, test = split(df, test_size = 0.33)\n",
        "\n",
        "\n",
        "\n",
        "scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "\n",
        "x = train.iloc[:,:-1].astype(float)\n",
        "y = train.iloc[:,-1]\n",
        "scalarX.fit(x)\n",
        "x = scalarX.transform(x)\n",
        "\n",
        "train = pd.concat([pd.DataFrame(x), y], axis = 1)\n",
        "train.columns = [\"x1\", \"x2\", \"cat1\",\"cat2\",\"cat3\",\"cat4\",\"cat5\",\"cat6\", \"y\"]\n",
        "\n",
        "x1 = test.iloc[:,:-1].values\n",
        "y1 = test.iloc[:,-1].values\n",
        "\n",
        "x1 = scalarX.transform(x1)\n",
        "\n",
        "test = pd.concat([pd.DataFrame(x1), pd.DataFrame(y1)], axis = 1)\n",
        "test.columns = [\"x1\", \"x2\", \"cat1\",\"cat2\",\"cat3\",\"cat4\",\"cat5\",\"cat6\", \"y\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-xZTS8FZklS8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = get_data2(observations = 100, sigma = 0.01, representation = \"nn\"  )\n",
        "\n",
        "x = np.array(df.iloc[:, :-1])\n",
        "y = np.array(df.iloc[:,-1])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.33) ## Split data into training and testing set\n",
        "      \n",
        "## Normalise  \n",
        "scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "scalarX.fit(X_train[:,:-1])\n",
        "\n",
        "X_train[:,:-1] = scalarX.transform(X_train[:,:-1])\n",
        "\n",
        "X_test[:,:-1] = scalarX.transform(X_test[:,:-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oNDo_0ddwWy1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df= get_cat_data2(observations = 100, sigma =0.01, representation = \"nn\")\n",
        "train, test = split(df, test_size = 0.33)\n",
        "\n",
        "scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
        "\n",
        "x = train.iloc[:,:-1].astype(float)\n",
        "y = train.iloc[:,-1]\n",
        "scalarX.fit(x)\n",
        "x = scalarX.transform(x)\n",
        "\n",
        "train = pd.concat([pd.DataFrame(x), y], axis = 1)\n",
        "train.columns = [\"x1\", \"x2\", \"cat\", \"y\"]\n",
        "\n",
        "x1 = test.iloc[:,:-1].values\n",
        "y1 = test.iloc[:,-1].values\n",
        "\n",
        "x1 = scalarX.transform(x1)\n",
        "\n",
        "test = pd.concat([pd.DataFrame(x1), pd.DataFrame(y1)], axis = 1)\n",
        "test.columns = [\"x1\", \"x2\", \"cat\", \"y\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LmeofBzqwWkR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "1zHHNfljkjdx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}